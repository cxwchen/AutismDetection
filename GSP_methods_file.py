#%%
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib as plt
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve
from classification.src import classifiers as cl, basicfeatureextraction
from featureselection.src.feature_selection_methods import *
from featuredesign.graph_inference.AAL_test import *
import os,json,glob,re,random, contextlib, io
from nilearn.datasets import fetch_abide_pcp
from dotenv import load_dotenv
from pathlib import Path

def cross_validate_model(X, y, n_splits=5):
    """
    Perform K-Fold cross-validation on the model and return the average accuracy.
    """
    kf = KFold(n_splits=n_splits, shuffle=True)
    acc_scores = []

    # Convert inputs to numpy arrays once at the beginning
    X = np.asarray(X, dtype=np.float64)
    y = np.asarray(y, dtype=np.float64).reshape(-1)

    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        # Train the model (example using SVM)
        model = cl.applySVM(X_train, y_train)
        y_pred = model.predict(X_test)

        # Compute accuracy for each fold
        acc = accuracy_score(y_test, y_pred)
        acc_scores.append(acc)

    avg_acc = np.mean(acc_scores)
    return avg_acc, acc_scores  # return both the average and individual fold scores

def load_and_process_data(file_path):
    """
    Load data and process it for classification.
    """
    full_df = pd.read_csv(file_path)
    full_df = full_df.sample(frac=1).reset_index(drop=True)  # Shuffle the DataFrame

    X = full_df.drop(columns=['DX_GROUP', 'subject_id', 'SEX', 'AGE_AT_SCAN'])
    y = full_df['DX_GROUP'].map({1: 1, 2: 0})  # 1 ASD, 0 ALL

    # Ensure that the data is numeric and handle missing values
    X = X.apply(pd.to_numeric, errors='coerce')
    X = X.dropna(axis=1, how='all')
    non_nan_ratio = X.notna().mean()
    X = X.loc[:, non_nan_ratio > 0.8]  # Keep columns with more than 80% non-NaN values
    X = X.loc[:, X.var() > 1e-6]  # Remove columns with low variance
    X = X.fillna(X.median())  # Fill NaN with the median

    return X, y, full_df, file_path

def parse_filename(file_name):
    """
    Parses a filename generated by generate_filename and returns [inf, cov, alpha, thresh].
    """
    # Remove the .csv extension if present
    if file_name.endswith('.csv'):
        file_name = file_name[:-4]
    parts = file_name.split('_')
    # inf is at index 4, cov at 5, alpha at 7, thresh at 8
    inf = parts[4].replace('-', '_')
    cov = parts[5]
    alpha = float(parts[7].replace('alpha', ''))
    thresh = float(parts[8].replace('thr', ''))
    return inf, cov, alpha, thresh

def plot_adjacency_matrix(df, subject_id, matrix_size=20):
    """
    This function takes the dataframe containing the flattened adjacency matrices,
    extracts the matrix for a specific subject, reshapes it, and plots the adjacency matrix.

    Parameters:
    - df: DataFrame containing the flattened adjacency matrices.
    - subject_id: The subject ID for which to plot the adjacency matrix.
    - matrix_size: The size of the square adjacency matrix (default is 20x20).
    """
    # Extract the row for the specific subject_id
    subject_row = df[df['subject_id'] == subject_id]

    # If the subject is not found in the DataFrame
    if subject_row.empty:
        print(f"Subject {subject_id} not found in the DataFrame.")
        return
    
    # Extract the flattened adjacency matrix values
    adj_values = subject_row.drop(columns=['subject_id', 'DX_GROUP', 'SEX', 'SITE_ID', 'AGE_AT_SCAN']).values.flatten()
    
    # Reshape the flattened array back into a square matrix
    adj_matrix = adj_values.reshape(matrix_size, matrix_size)
    
    # Plot the adjacency matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(adj_matrix, cmap="YlGnBu", annot=False, xticklabels=False, yticklabels=False)
    plt.title(f"Adjacency Matrix for Subject {subject_id}")
    plt.show()

def apply_threshold_to_file(input_file, output_file, threshold):
    """
    Loads a CSV file, applies thresholding to all adjacency matrix columns (A_{i}_{j}),
    setting values below threshold to 0, and saves the result to a new file
    in the Feature_Dataframes folder. Non-adjacency columns are left unchanged.
    """
    df = pd.read_csv(input_file)
    # Only apply threshold to columns that match the adjacency pattern
    adj_cols = [col for col in df.columns if re.match(r"A_\d+_\d+", col)]
    df[adj_cols] = df[adj_cols].applymap(lambda x: x if abs(x) >= threshold else 0)

    # Ensure output is saved in Feature_Dataframes
    feature_df_folder = os.path.join(os.path.abspath(os.path.dirname(__file__)), "Feature_Dataframes")
    os.makedirs(feature_df_folder, exist_ok=True)
    output_file = os.path.join(feature_df_folder, os.path.basename(output_file))

    df.to_csv(output_file, index=False)
    print(f"Thresholded file saved to {output_file}")

def plotting():
    results = []

    file_paths = select_dfs_by_params()

    for file_path in file_paths:
        X, y, full_df, file_path = load_and_process_data(file_path)
        inf, cov, alpha, thresh = parse_filename(os.path.basename(file_path))        # Plot the adjacency matrix for subject '0051044' in the current subplot
        subject_id_to_plot = 51044      # You can change this subject ID if needed
        #plot_adjacency_matrix(full_df, subject_id_to_plot)
        avg_acc, acc_scores = cross_validate_model(X, y)
        results.append((inf, cov, alpha, thresh, avg_acc))

    # Convert results into a numpy array for easy manipulation
    results_array = np.array(results, dtype=object)

    # Get unique sorted alpha and threshold values from results
    alpha_values = sorted(set(results_array[:, 2]))
    thresholds = sorted(set(results_array[:, 3]))

    # Set up the grid for plotting
    fig, ax = plt.subplots(figsize=(12, 8))

    # Create a grid based on alpha and threshold values
    grid = np.zeros((len(alpha_values), len(thresholds)))

    for i, alpha in enumerate(alpha_values):
        for j, thresh in enumerate(thresholds):
            # Extract the result corresponding to each (alpha, threshold) combination
            matching_results = results_array[(results_array[:, 2] == alpha) & (results_array[:, 3] == thresh)]
            if matching_results.size > 0:
                grid[i, j] = matching_results[0, 4]  # Store average accuracy

    # Plot the grid
    cax = ax.matshow(grid, cmap='viridis')
    fig.colorbar(cax)
    ax.set_xticks(np.arange(len(thresholds)))
    ax.set_xticklabels([f"{thresh:}" for thresh in thresholds])
    ax.set_yticks(np.arange(len(alpha_values)))
    ax.set_yticklabels([f"{alpha:}" for alpha in alpha_values])
    ax.set_xlabel('Threshold')
    ax.set_ylabel('Alpha')

    # Title and display
    ax.set_title("Average Accuracy for Varying Alpha and Thresholds")
    plt.show()
    return results_array

def batch_apply_thresholds_to_files():
    """
    For each selected file, applies a range of thresholds (0.05 to 0.95 step 0.05)
    to all adjacency matrix columns and saves the result as a new file for each threshold.
    """
    file_paths = select_dfs_by_params()
    if not file_paths:
        print("No files selected.")
        return

    thresholds = np.arange(0.05, 0.51, 0.05)
    for input_file in file_paths:
        base, ext = os.path.splitext(input_file)
        for threshold in thresholds:
            # Remove any _thr followed by digits, optional decimal, and more digits at the end of the base
            base = re.sub(r'_thr\d+(\.\d+)?$', '', base)
            output_file = f"{base}_thr{threshold:.2f}{ext}"
            apply_threshold_to_file(input_file, output_file, threshold)
    print("Batch thresholding complete.")

def compute_subjectwise_adjacency_distances_by_alpha(file_paths):
    """
    For each subject, computes the Frobenius distance between their adjacency matrices
    for each pair of consecutive alpha values. Then averages these distances over all subjects
    and plots the average distance for each alpha (except the last).
    """
    # Parse alpha for each file and load all adjacency matrices
    alpha_to_df = {}
    for file_path in file_paths:
        try:
            inf, cov, alpha, thresh = parse_filename(os.path.basename(file_path))
        except Exception as e:
            print(f"Could not parse alpha from {file_path}: {e}")
            continue
        df = pd.read_csv(file_path)
        alpha_to_df[alpha] = df

    sorted_alphas = sorted(alpha_to_df.keys())
    if not sorted_alphas:
        print("No valid alpha values found.")
        return

    # Get all subject IDs present in all files
    subject_sets = [set(df['subject_id']) for df in alpha_to_df.values()]
    common_subjects = set.intersection(*subject_sets)
    if not common_subjects:
        print("No common subjects found across all alpha files.")
        return

    # For each subject, collect their adjacency matrices for each alpha
    subject_distances = {alpha: [] for alpha in sorted_alphas[:-1]}  # Only up to the second last alpha
    adj_col_pattern = re.compile(r"A_\d+_\d+" )

    for subject_id in common_subjects:
        subject_matrices = []
        for alpha in sorted_alphas:
            df = alpha_to_df[alpha]
            row = df[df['subject_id'] == subject_id]
            if row.empty:
                break
            adj_cols = [col for col in df.columns if adj_col_pattern.match(col)]
            adj_values = row[adj_cols].values.flatten()
            n = int(np.sqrt(len(adj_cols)))
            adj_matrix = adj_values.reshape(n, n)
            subject_matrices.append(adj_matrix)
        if len(subject_matrices) == len(sorted_alphas):
            # Compute distance only between each alpha and the next bigger alpha
            for i in range(len(sorted_alphas) - 1):
                alpha1 = sorted_alphas[i]
                dist = np.linalg.norm(subject_matrices[i] - subject_matrices[i + 1], ord='fro')
                subject_distances[alpha1].append(dist)

    # Compute average distance for each alpha over all subjects
    avg_distances = []
    for alpha in sorted_alphas[:-1]:
        if subject_distances[alpha]:
            avg_distances.append(np.mean(subject_distances[alpha]))
        else:
            avg_distances.append(0)

    # Plot
    import matplotlib.pyplot as plt
    plt.figure(figsize=(8, 5))
    plt.plot(sorted_alphas[:-1], avg_distances, marker='o')
    plt.xscale('log')
    plt.xlabel('Alpha')
    plt.ylabel('Average Frobenius Distance to Next Alpha (per subject)')
    plt.title('Average Subjectwise Distance Between Adjacency Matrices for Consecutive Alpha Values')
    plt.grid(True)
    plt.show()

    return sorted_alphas[:-1], avg_distances

def plot_distance_distribution_by_alpha(file_paths):
    """
    For each alpha, plot the distribution (boxplot and histogram) of subjectwise adjacency distances
    between consecutive alpha values.
    """
    # Reuse the logic from compute_subjectwise_adjacency_distances_by_alpha to get the distances
    alpha_to_df = {}
    for file_path in file_paths:
        try:
            inf, cov, alpha, thresh = parse_filename(os.path.basename(file_path))
        except Exception as e:
            print(f"Could not parse alpha from {file_path}: {e}")
            continue
        df = pd.read_csv(file_path)
        alpha_to_df[alpha] = df

    sorted_alphas = sorted(alpha_to_df.keys())
    if not sorted_alphas:
        print("No valid alpha values found.")
        return

    subject_sets = [set(df['subject_id']) for df in alpha_to_df.values()]
    common_subjects = set.intersection(*subject_sets)
    if not common_subjects:
        print("No common subjects found across all alpha files.")
        return

    subject_distances = {alpha: [] for alpha in sorted_alphas[:-1]}
    adj_col_pattern = re.compile(r"A_\d+_\d+" )

    for subject_id in common_subjects:
        subject_matrices = []
        for alpha in sorted_alphas:
            df = alpha_to_df[alpha]
            row = df[df['subject_id'] == subject_id]
            if row.empty:
                break
            adj_cols = [col for col in df.columns if adj_col_pattern.match(col)]
            adj_values = row[adj_cols].values.flatten()
            n = int(np.sqrt(len(adj_cols)))
            adj_matrix = adj_values.reshape(n, n)
            subject_matrices.append(adj_matrix)
        if len(subject_matrices) == len(sorted_alphas):
            for i in range(len(sorted_alphas) - 1):
                alpha1 = sorted_alphas[i]
                dist = np.linalg.norm(subject_matrices[i] - subject_matrices[i + 1], ord='fro')
                subject_distances[alpha1].append(dist)

    # Plot boxplots for each alpha
    plt.figure(figsize=(12, 6))
    data = [subject_distances[alpha] for alpha in sorted_alphas[:-1]]
    plt.boxplot(data, labels=[f"{alpha:.4f}" for alpha in sorted_alphas[:-1]])
    plt.xlabel('Alpha')
    plt.ylabel('Frobenius Distance (subjectwise)')
    plt.title('Distribution of Subjectwise Adjacency Distances Between Consecutive Alpha Values')
    plt.grid(True)
    plt.show()

    # Optionally, plot histograms for each alpha
    plt.figure(figsize=(12, 6))
    for i, alpha in enumerate(sorted_alphas[:-1]):
        plt.hist(subject_distances[alpha], bins=20, alpha=0.5, label=f"alpha={alpha:.4f}")
    plt.xlabel('Frobenius Distance')
    plt.ylabel('Count')
    plt.title('Histogram of Subjectwise Adjacency Distances for Each Alpha')
    plt.legend()
    plt.show()

    return subject_distances

def average_longform_heatmaps_and_plot(file_paths, output_file=None):
    """
    Takes as input multiple CSV files in long-form (alpha, threshold, value),
    computes the average value for each (alpha, threshold) pair across all files,
    and plots/saves the resulting heatmap.
    """
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt

    dfs = []
    for file_path in file_paths:
        df = pd.read_csv(file_path, header=None)
        # Skip header if present
        df = df.iloc[1:]
        # The columns are: ?, ?, alpha, threshold, value
        df.columns = ['col0', 'col1', 'alpha', 'threshold', 'value']
        df['alpha'] = pd.to_numeric(df['alpha'], errors='coerce')
        df['threshold'] = pd.to_numeric(df['threshold'], errors='coerce')
        df['value'] = pd.to_numeric(df['value'], errors='coerce')
        dfs.append(df[['alpha', 'threshold', 'value']])
        

    # Concatenate and group by (alpha, threshold)
    all_df = pd.concat(dfs)
    avg_df = all_df.groupby(['alpha', 'threshold'], as_index=False)['value'].mean()
    
    # Pivot to matrix form for heatmap
    heatmap_df = avg_df.pivot(index='alpha', columns='threshold', values='value')
    heatmap_arr = heatmap_df.values
    print(heatmap_arr)
    # Plot
    
    plt.figure(figsize=(10, 8))
    plt.imshow(heatmap_arr, cmap='viridis')
    plt.colorbar(label='Average Value')
    plt.title('Average Heatmap of 5 runs of 5-fold cross-validation')
    plt.xlabel('Threshold')
    plt.ylabel('Alpha')
    plt.xticks(ticks=np.arange(len(heatmap_df.columns)), labels=[f"{x:.2f}" for x in heatmap_df.columns])
    plt.yticks(ticks=np.arange(len(heatmap_df.index)), labels=[f"{x:.4f}" for x in heatmap_df.index])
    plt.tight_layout()
    plt.show()

    # Optionally save the averaged table
    if output_file:
        heatmap_df.to_csv(output_file)
        print(f"Averaged heatmap table saved to {output_file}")

    return heatmap_df

def select_dfs_by_params():
    """
    Allows the user to select inf_method, alpha, and thresh (or 'all' for any),
    then returns a list of matching dataframe file paths.
    """
    folder_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), "Feature_Dataframes")
    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]
    parsed = []
    for f in files:
        try:
            inf, cov, alpha, thresh = parse_filename(f)
            parsed.append({'file': f, 'inf': inf, 'alpha': alpha, 'thresh': thresh})
        except Exception:
            continue

    # Get unique values for each parameter
    inf_methods = sorted(set(p['inf'] for p in parsed))
    alphas = sorted(set(p['alpha'] for p in parsed))
    threshes = sorted(set(p['thresh'] for p in parsed))

    # Prompt user for selection
    print("Available inf_methods:", inf_methods)
    inf_choice = input("Select inf_method (or type 'all'): ").strip()
    print("Available alphas:", alphas)
    alpha_choice = input("Select alpha (or type 'all'): ").strip()
    print("Available threshes:", threshes)
    thresh_choice = input("Select thresh (or type 'all'): ").strip()

    # Convert choices
    inf_selected = inf_methods if inf_choice.lower() == 'all' else [inf_choice]
    alpha_selected = alphas if alpha_choice.lower() == 'all' else [float(alpha_choice)]
    thresh_selected = threshes if thresh_choice.lower() == 'all' else [float(thresh_choice)]

    # Find matching files
    selected_files = [
        os.path.join(folder_path, p['file'])
        for p in parsed
        if p['inf'] in inf_selected and p['alpha'] in alpha_selected and p['thresh'] in thresh_selected
    ]
    if not selected_files:
        print("No files match the selected criteria.")
    else:
        print("Selected files:")
        for f in selected_files:
            print(f)
    return selected_files

def select_single_file():
    """
    Allows the user to select a single file from all available files in the Feature_Dataframes folder.
    Returns the selected file path or None if no selection is made.
    """
    folder_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), "Feature_Dataframes")
    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]
    if not files:
        print("No files found in Feature_Dataframes.")
        return None

    print("Available files:")
    for idx, f in enumerate(files):
        print(f"{idx}: {f}")

    selection = input("Enter the number of the file you want to select: ").strip()
    try:
        selection_idx = int(selection)
        if 0 <= selection_idx < len(files):
            selected_file = os.path.join(folder_path, files[selection_idx])
            print(f"Selected file: {selected_file}")
            return selected_file
        else:
            print("Invalid selection.")
            return None
    except ValueError:
        print("Invalid input.")
        return None

def select_multiple_files():
    """
    Allows the user to select multiple files from all available files in the Feature_Dataframes folder.
    Returns a list of selected file paths.
    """
    folder_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), "Feature_Dataframes")
    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]
    if not files:
        print("No files found in Feature_Dataframes.")
        return []

    print("Available files:")
    for idx, f in enumerate(files):
        print(f"{idx}: {f}")

    selection = input("Enter the numbers of the files you want to select, separated by commas (e.g., 0,2,5): ").strip()
    try:
        indices = [int(idx.strip()) for idx in selection.split(",") if idx.strip().isdigit()]
        selected_files = []
        for idx in indices:
            if 0 <= idx < len(files):
                selected_files.append(os.path.join(folder_path, files[idx]))
            else:
                print(f"Index {idx} is out of range and will be skipped.")
        if selected_files:
            print("Selected files:")
            for f in selected_files:
                print(f)
        else:
            print("No valid files selected.")
        return selected_files
    except Exception as e:
        print(f"Invalid input: {e}")
        return []

def plot_subject_adjacency_across_files(file_paths, subject_id=51044, matrix_size=20):
    """
    For each file in file_paths, plot the adjacency matrix for the given subject_id.
    """
    for file_path in file_paths:
        df = pd.read_csv(file_path)
        # Extract the row for the specific subject_id
        subject_row = df[df['subject_id'] == subject_id]
        if subject_row.empty:
            print(f"Subject {subject_id} not found in {os.path.basename(file_path)}.")
            continue

        # Extract adjacency matrix columns (assumes columns named like A_0_0, A_0_1, ...)
        adj_cols = [col for col in df.columns if re.match(r"A_\d+_\d+", col)]
        adj_values = subject_row[adj_cols].values.flatten()
        n = int(np.sqrt(len(adj_cols))) if matrix_size is None else matrix_size
        adj_matrix = adj_values.reshape(n, n)

        plt.figure(figsize=(8, 6))
        sns.heatmap(adj_matrix, cmap="YlGnBu", annot=False, xticklabels=False, yticklabels=False)
        plt.title(f"Adjacency Matrix for Subject {subject_id}\nFile: {os.path.basename(file_path)}")
        plt.show()
        return adj_matrix  # Return the last plotted adjacency matrix for further processing if needed

def plot_normalized_laplacian(W):
    """
    Compute the normalized Laplacian matrix from an adjacency matrix W,
    mask out the diagonal, and plot the result.
    L_norm = I - D^{-1/2} W D^{-1/2}
    """
    D = np.diag(W.sum(axis=1))
    D_inv_sqrt = np.diag(1.0 / np.sqrt(np.maximum(D.diagonal(), 1e-10)))
    L_norm = np.eye(W.shape[0]) - D_inv_sqrt @ W @ D_inv_sqrt

    np.fill_diagonal(L_norm, 0)

    # Compute the max absolute value in the thresholded matrix
    max_abs_value = np.max(np.abs(L_norm))
        
    # Avoid division by zero if the max absolute value is 0
    if max_abs_value != 0:
        L_norm_norm = L_norm / max_abs_value
    else:
        print("Warning: Maximum absolute value is 0, normalization skipped.")

    plt.figure(figsize=(8, 6))
    sns.heatmap(L_norm_norm, cmap="YlGnBu", annot=False, xticklabels=False, yticklabels=False)
    plt.title("Normalized Laplacian (Diagonal Masked)")
    plt.show()

def plot_subject_covariance_across_files(file_paths, subject_id=51044, matrix_size=20):
    """
    For each file in file_paths, plot the covariance matrix for the given subject_id.
    Assumes columns are named like C_0_0, C_0_1, ..., C_n_n for covariance entries.
    """

    for file_path in file_paths:
        df = pd.read_csv(file_path)
        # Extract the row for the specific subject_id
        subject_row = df[df['subject_id'] == subject_id]
        if subject_row.empty:
            print(f"Subject {subject_id} not found in {os.path.basename(file_path)}.")
            continue

        # Extract covariance matrix columns (assumes columns named like C_0_0, C_0_1, ...)
        cov_cols = [col for col in df.columns if re.match(r"C_\d+_\d+", col)]
        cov_values = subject_row[cov_cols].values.flatten()
        n = int(np.sqrt(len(cov_cols))) if matrix_size is None else matrix_size
        cov_matrix = cov_values.reshape(n, n)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cov_matrix, cmap="coolwarm", annot=False, xticklabels=False, yticklabels=False)
        plt.title(f"Covariance Matrix for Subject {subject_id}\nFile: {os.path.basename(file_path)}")
        plt.show()
        return cov_matrix  # Return the last plotted covariance matrix for further processing if needed

if __name__ == "__main__":
    file_paths = select_dfs_by_params()
    adj_matrix = plot_subject_adjacency_across_files(file_paths)
    plot_normalized_laplacian(adj_matrix)
    #file_path = select_multiple_files()
    #plot_subject_covariance_across_files(file_path)
    #results_array=plotting()
    #batch_apply_thresholds_to_files()
    #plot_distance_distribution_by_alpha(file_paths)
    #compute_subjectwise_adjacency_distances_by_alpha(file_paths)
    #plot_distance_distribution_by_alpha(file_paths)
    #average_longform_heatmaps_and_plot(file_paths)
# %%

