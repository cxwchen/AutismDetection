#%%
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib as plt
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve
from classification.src import classifiers as cl, basicfeatureextraction
from featureselection.src.feature_selection_methods import *
from featuredesign.graph_inference.AAL_test import *
import os,json,glob,re,random, contextlib, io
from nilearn.datasets import fetch_abide_pcp
from dotenv import load_dotenv
from pathlib import Path

def cross_validate_model(X, y, n_splits=5):
    """
    Perform K-Fold cross-validation on the model and return the average accuracy.
    """
    kf = KFold(n_splits=n_splits, shuffle=True)
    acc_scores = []

    # Convert inputs to numpy arrays once at the beginning
    X = np.asarray(X, dtype=np.float64)
    y = np.asarray(y, dtype=np.float64).reshape(-1)

    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        # Train the model (example using SVM)
        model = cl.applySVM(X_train, y_train)
        y_pred = model.predict(X_test)

        # Compute accuracy for each fold
        acc = accuracy_score(y_test, y_pred)
        acc_scores.append(acc)

    avg_acc = np.mean(acc_scores)
    return avg_acc, acc_scores  # return both the average and individual fold scores

def load_dfs():
    """
    Let the user select a range of files from the 'Feature_Dataframes' folder and return their paths.
    The user can input a single number, a comma-separated list, or a range (e.g., 2-5).
    """
    folder_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), "Feature_Dataframes")
    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]
    
    if not files:
        print("No files found in the folder.")
        return []

    print("Files in folder:")
    for idx, filename in enumerate(files):
        print(f"{idx + 1}: {filename}")

    while True:
        try:
            choices = input("Enter file numbers to select (e.g. 1,3,5-8): ")
            indices = []
            for part in choices.split(','):
                part = part.strip()
                if '-' in part:
                    start, end = part.split('-')
                    indices.extend(range(int(start), int(end) + 1))
                elif part:
                    indices.append(int(part))
            if all(1 <= idx <= len(files) for idx in indices):
                selected_files = [os.path.join(folder_path, files[idx - 1]) for idx in indices]
                print("Selected files:")
                for f in selected_files:
                    print(f)
                return selected_files
            else:
                print("Invalid number(s). Try again.")
        except ValueError:
            print("Please enter valid numbers, separated by commas or ranges (e.g. 1,3,5-8).")

def load_and_process_data(file_path):
    """
    Load data and process it for classification.
    """
    full_df = pd.read_csv(file_path)
    full_df = full_df.sample(frac=1).reset_index(drop=True)  # Shuffle the DataFrame

    X = full_df.drop(columns=['DX_GROUP', 'subject_id', 'SEX', 'AGE_AT_SCAN'])
    y = full_df['DX_GROUP'].map({1: 1, 2: 0})  # 1 ASD, 0 ALL

    # Ensure that the data is numeric and handle missing values
    X = X.apply(pd.to_numeric, errors='coerce')
    X = X.dropna(axis=1, how='all')
    non_nan_ratio = X.notna().mean()
    X = X.loc[:, non_nan_ratio > 0.8]  # Keep columns with more than 80% non-NaN values
    X = X.loc[:, X.var() > 1e-6]  # Remove columns with low variance
    X = X.fillna(X.median())  # Fill NaN with the median

    return X, y, full_df, file_path

def parse_filename(file_name):
    """
    Parses a filename generated by generate_filename and returns [inf, cov, alpha, thresh].
    """
    # Remove the .csv extension if present
    if file_name.endswith('.csv'):
        file_name = file_name[:-4]
    parts = file_name.split('_')
    # inf is at index 4, cov at 5, alpha at 7, thresh at 8
    inf = parts[4].replace('-', '_')
    cov = parts[5]
    alpha = float(parts[7].replace('alpha', ''))
    thresh = float(parts[8].replace('thr', ''))
    return inf, cov, alpha, thresh

def plot_adjacency_matrix(df, subject_id, matrix_size=20):
    """
    This function takes the dataframe containing the flattened adjacency matrices,
    extracts the matrix for a specific subject, reshapes it, and plots the adjacency matrix.

    Parameters:
    - df: DataFrame containing the flattened adjacency matrices.
    - subject_id: The subject ID for which to plot the adjacency matrix.
    - matrix_size: The size of the square adjacency matrix (default is 20x20).
    """
    # Extract the row for the specific subject_id
    subject_row = df[df['subject_id'] == subject_id]

    # If the subject is not found in the DataFrame
    if subject_row.empty:
        print(f"Subject {subject_id} not found in the DataFrame.")
        return
    
    # Extract the flattened adjacency matrix values
    adj_values = subject_row.drop(columns=['subject_id', 'DX_GROUP', 'SEX', 'SITE_ID', 'AGE_AT_SCAN']).values.flatten()
    
    # Reshape the flattened array back into a square matrix
    adj_matrix = adj_values.reshape(matrix_size, matrix_size)
    
    # Plot the adjacency matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(adj_matrix, cmap="YlGnBu", annot=False, xticklabels=False, yticklabels=False)
    plt.title(f"Adjacency Matrix for Subject {subject_id}")
    plt.show()

def apply_threshold_to_file(input_file, output_file, threshold):
    """
    Loads a CSV file, applies thresholding to all adjacency matrix columns (A_{i}_{j}),
    setting values below threshold to 0, and saves the result to a new file
    in the Feature_Dataframes folder. Non-adjacency columns are left unchanged.
    """
    df = pd.read_csv(input_file)
    # Only apply threshold to columns that match the adjacency pattern
    adj_cols = [col for col in df.columns if re.match(r"A_\d+_\d+", col)]
    df[adj_cols] = df[adj_cols].applymap(lambda x: x if abs(x) >= threshold else 0)

    # Ensure output is saved in Feature_Dataframes
    feature_df_folder = os.path.join(os.path.abspath(os.path.dirname(__file__)), "Feature_Dataframes")
    os.makedirs(feature_df_folder, exist_ok=True)
    output_file = os.path.join(feature_df_folder, os.path.basename(output_file))

    df.to_csv(output_file, index=False)
    print(f"Thresholded file saved to {output_file}")

def plotting():
    results = []

    file_paths = load_dfs()

    for file_path in file_paths:
        X, y, full_df, file_path = load_and_process_data(file_path)
        inf, cov, alpha, thresh = parse_filename(file_path)
        # Plot the adjacency matrix for subject '0051044' in the current subplot
        subject_id_to_plot = 51044      # You can change this subject ID if needed
        #plot_adjacency_matrix(full_df, subject_id_to_plot)
        avg_acc, acc_scores = cross_validate_model(X, y)
        results.append((inf, cov, alpha, thresh, avg_acc))

    # Convert results into a numpy array for easy manipulation
    results_array = np.array(results, dtype=object)

    # Get unique sorted alpha and threshold values from results
    alpha_values = sorted(set(results_array[:, 2]))
    thresholds = sorted(set(results_array[:, 3]))

    # Set up the grid for plotting
    fig, ax = plt.subplots(figsize=(12, 8))

    # Create a grid based on alpha and threshold values
    grid = np.zeros((len(alpha_values), len(thresholds)))

    for i, alpha in enumerate(alpha_values):
        for j, thresh in enumerate(thresholds):
            # Extract the result corresponding to each (alpha, threshold) combination
            matching_results = results_array[(results_array[:, 2] == alpha) & (results_array[:, 3] == thresh)]
            if matching_results.size > 0:
                grid[i, j] = matching_results[0, 4]  # Store average accuracy

    # Plot the grid
    cax = ax.matshow(grid, cmap='viridis')
    fig.colorbar(cax)
    ax.set_xticks(np.arange(len(thresholds)))
    ax.set_xticklabels([f"{thresh:}" for thresh in thresholds])
    ax.set_yticks(np.arange(len(alpha_values)))
    ax.set_yticklabels([f"{alpha:}" for alpha in alpha_values])
    ax.set_xlabel('Threshold')
    ax.set_ylabel('Alpha')

    # Title and display
    ax.set_title("Average Accuracy for Varying Alpha and Thresholds")
    plt.show()
    return results_array

def batch_apply_thresholds_to_files():
    """
    For each selected file, applies a range of thresholds (0.05 to 0.95 step 0.05)
    to all adjacency matrix columns and saves the result as a new file for each threshold.
    """
    file_paths = load_dfs()
    if not file_paths:
        print("No files selected.")
        return

    thresholds = np.arange(0.05, 0.51, 0.05)
    for input_file in file_paths:
        base, ext = os.path.splitext(input_file)
        for threshold in thresholds:
            # Remove any _thr followed by digits, optional decimal, and more digits at the end of the base
            base = re.sub(r'_thr\d+(\.\d+)?$', '', base)
            output_file = f"{base}_thr{threshold:.2f}{ext}"
            apply_threshold_to_file(input_file, output_file, threshold)
    print("Batch thresholding complete.")

def compute_subjectwise_adjacency_distances_by_alpha(file_paths):
    """
    For each subject, computes the Frobenius distance between their adjacency matrices
    for each pair of consecutive alpha values. Then averages these distances over all subjects
    and plots the average distance for each alpha (except the last).
    """
    # Parse alpha for each file and load all adjacency matrices
    alpha_to_df = {}
    for file_path in file_paths:
        try:
            inf, cov, alpha, thresh = parse_filename(os.path.basename(file_path))
        except Exception as e:
            print(f"Could not parse alpha from {file_path}: {e}")
            continue
        df = pd.read_csv(file_path)
        alpha_to_df[alpha] = df

    sorted_alphas = sorted(alpha_to_df.keys())
    if not sorted_alphas:
        print("No valid alpha values found.")
        return

    # Get all subject IDs present in all files
    subject_sets = [set(df['subject_id']) for df in alpha_to_df.values()]
    common_subjects = set.intersection(*subject_sets)
    if not common_subjects:
        print("No common subjects found across all alpha files.")
        return

    # For each subject, collect their adjacency matrices for each alpha
    subject_distances = {alpha: [] for alpha in sorted_alphas[:-1]}  # Only up to the second last alpha
    adj_col_pattern = re.compile(r"A_\d+_\d+" )

    for subject_id in common_subjects:
        subject_matrices = []
        for alpha in sorted_alphas:
            df = alpha_to_df[alpha]
            row = df[df['subject_id'] == subject_id]
            if row.empty:
                break
            adj_cols = [col for col in df.columns if adj_col_pattern.match(col)]
            adj_values = row[adj_cols].values.flatten()
            n = int(np.sqrt(len(adj_cols)))
            adj_matrix = adj_values.reshape(n, n)
            subject_matrices.append(adj_matrix)
        if len(subject_matrices) == len(sorted_alphas):
            # Compute distance only between each alpha and the next bigger alpha
            for i in range(len(sorted_alphas) - 1):
                alpha1 = sorted_alphas[i]
                dist = np.linalg.norm(subject_matrices[i] - subject_matrices[i + 1], ord='fro')
                subject_distances[alpha1].append(dist)

    # Compute average distance for each alpha over all subjects
    avg_distances = []
    for alpha in sorted_alphas[:-1]:
        if subject_distances[alpha]:
            avg_distances.append(np.mean(subject_distances[alpha]))
        else:
            avg_distances.append(0)

    # Plot
    import matplotlib.pyplot as plt
    plt.figure(figsize=(8, 5))
    plt.plot(sorted_alphas[:-1], avg_distances, marker='o')
    plt.xscale('log')
    plt.xlabel('Alpha')
    plt.ylabel('Average Frobenius Distance to Next Alpha (per subject)')
    plt.title('Average Subjectwise Distance Between Adjacency Matrices for Consecutive Alpha Values')
    plt.grid(True)
    plt.show()

    return sorted_alphas[:-1], avg_distances

def plot_distance_distribution_by_alpha(file_paths):
    """
    For each alpha, plot the distribution (boxplot and histogram) of subjectwise adjacency distances
    between consecutive alpha values.
    """
    # Reuse the logic from compute_subjectwise_adjacency_distances_by_alpha to get the distances
    alpha_to_df = {}
    for file_path in file_paths:
        try:
            inf, cov, alpha, thresh = parse_filename(os.path.basename(file_path))
        except Exception as e:
            print(f"Could not parse alpha from {file_path}: {e}")
            continue
        df = pd.read_csv(file_path)
        alpha_to_df[alpha] = df

    sorted_alphas = sorted(alpha_to_df.keys())
    if not sorted_alphas:
        print("No valid alpha values found.")
        return

    subject_sets = [set(df['subject_id']) for df in alpha_to_df.values()]
    common_subjects = set.intersection(*subject_sets)
    if not common_subjects:
        print("No common subjects found across all alpha files.")
        return

    subject_distances = {alpha: [] for alpha in sorted_alphas[:-1]}
    adj_col_pattern = re.compile(r"A_\d+_\d+" )

    for subject_id in common_subjects:
        subject_matrices = []
        for alpha in sorted_alphas:
            df = alpha_to_df[alpha]
            row = df[df['subject_id'] == subject_id]
            if row.empty:
                break
            adj_cols = [col for col in df.columns if adj_col_pattern.match(col)]
            adj_values = row[adj_cols].values.flatten()
            n = int(np.sqrt(len(adj_cols)))
            adj_matrix = adj_values.reshape(n, n)
            subject_matrices.append(adj_matrix)
        if len(subject_matrices) == len(sorted_alphas):
            for i in range(len(sorted_alphas) - 1):
                alpha1 = sorted_alphas[i]
                dist = np.linalg.norm(subject_matrices[i] - subject_matrices[i + 1], ord='fro')
                subject_distances[alpha1].append(dist)

    # Plot boxplots for each alpha
    plt.figure(figsize=(12, 6))
    data = [subject_distances[alpha] for alpha in sorted_alphas[:-1]]
    plt.boxplot(data, labels=[f"{alpha:.4f}" for alpha in sorted_alphas[:-1]])
    plt.xlabel('Alpha')
    plt.ylabel('Frobenius Distance (subjectwise)')
    plt.title('Distribution of Subjectwise Adjacency Distances Between Consecutive Alpha Values')
    plt.grid(True)
    plt.show()

    # Optionally, plot histograms for each alpha
    plt.figure(figsize=(12, 6))
    for i, alpha in enumerate(sorted_alphas[:-1]):
        plt.hist(subject_distances[alpha], bins=20, alpha=0.5, label=f"alpha={alpha:.4f}")
    plt.xlabel('Frobenius Distance')
    plt.ylabel('Count')
    plt.title('Histogram of Subjectwise Adjacency Distances for Each Alpha')
    plt.legend()
    plt.show()

    return subject_distances

def average_longform_heatmaps_and_plot(file_paths, output_file=None):
    """
    Takes as input multiple CSV files in long-form (alpha, threshold, value),
    computes the average value for each (alpha, threshold) pair across all files,
    and plots/saves the resulting heatmap.
    """
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt

    dfs = []
    for file_path in file_paths:
        df = pd.read_csv(file_path, header=None)
        # Skip header if present
        df = df.iloc[1:]
        # The columns are: ?, ?, alpha, threshold, value
        df.columns = ['col0', 'col1', 'alpha', 'threshold', 'value']
        df['alpha'] = pd.to_numeric(df['alpha'], errors='coerce')
        df['threshold'] = pd.to_numeric(df['threshold'], errors='coerce')
        df['value'] = pd.to_numeric(df['value'], errors='coerce')
        dfs.append(df[['alpha', 'threshold', 'value']])
        

    # Concatenate and group by (alpha, threshold)
    all_df = pd.concat(dfs)
    avg_df = all_df.groupby(['alpha', 'threshold'], as_index=False)['value'].mean()
    
    # Pivot to matrix form for heatmap
    heatmap_df = avg_df.pivot(index='alpha', columns='threshold', values='value')
    heatmap_arr = heatmap_df.values
    print(heatmap_arr)
    # Plot
    
    plt.figure(figsize=(10, 8))
    plt.imshow(heatmap_arr, cmap='viridis')
    plt.colorbar(label='Average Value')
    plt.title('Average Heatmap of 5 runs of 5-fold cross-validation')
    plt.xlabel('Threshold')
    plt.ylabel('Alpha')
    plt.xticks(ticks=np.arange(len(heatmap_df.columns)), labels=[f"{x:.2f}" for x in heatmap_df.columns])
    plt.yticks(ticks=np.arange(len(heatmap_df.index)), labels=[f"{x:.4f}" for x in heatmap_df.index])
    plt.tight_layout()
    plt.show()

    # Optionally save the averaged table
    if output_file:
        heatmap_df.to_csv(output_file)
        print(f"Averaged heatmap table saved to {output_file}")

    return heatmap_df

def select_dfs_by_params():
    """
    Allows the user to select inf_method, alpha, and thresh (or 'all' for any),
    then returns a list of matching dataframe file paths.
    """
    folder_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), "Feature_Dataframes")
    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]
    parsed = []
    for f in files:
        try:
            inf, cov, alpha, thresh = parse_filename(f)
            parsed.append({'file': f, 'inf': inf, 'alpha': alpha, 'thresh': thresh})
        except Exception:
            continue

    # Get unique values for each parameter
    inf_methods = sorted(set(p['inf'] for p in parsed))
    alphas = sorted(set(p['alpha'] for p in parsed))
    threshes = sorted(set(p['thresh'] for p in parsed))

    # Prompt user for selection
    print("Available inf_methods:", inf_methods)
    inf_choice = input("Select inf_method (or type 'all'): ").strip()
    print("Available alphas:", alphas)
    alpha_choice = input("Select alpha (or type 'all'): ").strip()
    print("Available threshes:", threshes)
    thresh_choice = input("Select thresh (or type 'all'): ").strip()

    # Convert choices
    inf_selected = inf_methods if inf_choice.lower() == 'all' else [inf_choice]
    alpha_selected = alphas if alpha_choice.lower() == 'all' else [float(alpha_choice)]
    thresh_selected = threshes if thresh_choice.lower() == 'all' else [float(thresh_choice)]

    # Find matching files
    selected_files = [
        os.path.join(folder_path, p['file'])
        for p in parsed
        if p['inf'] in inf_selected and p['alpha'] in alpha_selected and p['thresh'] in thresh_selected
    ]
    if not selected_files:
        print("No files match the selected criteria.")
    else:
        print("Selected files:")
        for f in selected_files:
            print(f)
    return selected_files

# Example usage:
# file_paths = ['heatmap1.csv', 'heatmap2.csv', 'heatmap3.csv']
# average_heatmaps_and_plot(file_paths, output_file='average_heatmap.csv')

if __name__ == "__main__":
    file_paths = select_dfs_by_params()
    #results_array=plotting()
    plot_distance_distribution_by_alpha(file_paths)
    #compute_subjectwise_adjacency_distances_by_alpha(file_paths)
    #plot_distance_distribution_by_alpha(file_paths)
    #average_longform_heatmaps_and_plot(file_paths)